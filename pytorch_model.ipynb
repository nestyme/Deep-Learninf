{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "## Семинар 6\n",
    "### Классификация изображений на примере фотографий котов и собак.\n",
    "## DLSchool\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мотивировка:** В 2010-х годах компанией $Google$ в качестве подтверждения того, что пользователь -- не робот, предлагалось из некоторого набора картинок выборать всех собак или всех котов. Если какой-нибудь питомец особенно понравился, можно было пройти по ссылке и забрать его из приюта. \n",
    "Но прогресс не стоял на месте, нейронные сети научились проходить \"тест на человека\". Такую сеть мы сейчас и рассмотрим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import shutil\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пути по директориям\n",
    "data_path = DATA_HOME_DIR + '/' \n",
    "split_train_path = data_path + '/train/'\n",
    "full_train_path = data_path + '/train_full/'\n",
    "valid_path = data_path + '/valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/test/'\n",
    "saved_model_path = ROOT_DIR + '/models/'\n",
    "submission_path = ROOT_DIR + '/submissions/'\n",
    "\n",
    "# данные\n",
    "batch_size = 8\n",
    "nb_split_train_samples = 23000\n",
    "nb_full_train_samples = 25000\n",
    "nb_valid_samples = 2000\n",
    "nb_test_samples = 12500\n",
    "\n",
    "# параметры модели\n",
    "nb_runs = 1\n",
    "nb_aug = 3\n",
    "epochs = 35\n",
    "lr = 1e-4\n",
    "clip = 0.001\n",
    "archs = [\"resnet152\"]\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__ if name.islower() and not name.startswith(\"__\"))\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** что лежит внутри model_names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Некоторые вспомогательные функции \n",
    "**Вопрос: что конкретно делает каждая из функций? Можно ли как-то заоптимизировать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    # переключение на обучение\n",
    "    model.train()\n",
    "    \n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # время загрузки\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        image_var = torch.autograd.Variable(images)\n",
    "        label_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # вычисление y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # вычисляем точность и ошибкую \n",
    "        prec1, prec1 = accuracy(y_pred.data, target, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # Градиент\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # оцениваем модель\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        labels = labels.cuda(async=True)\n",
    "        image_var = torch.autograd.Variable(images, volatile=True)\n",
    "        label_var = torch.autograd.Variable(labels, volatile=True)\n",
    "\n",
    "        # снова вычисляем y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # снова точность и ошибка\n",
    "        prec1, temp_var = accuracy(y_pred.data, labels, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    return acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model):\n",
    "    csv_map = collections.defaultdict(float)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for aug in range(nb_aug):\n",
    "        print(\"   * Предсказание увеличения {}\".format(aug + 1))\n",
    "        \n",
    "        for i, (images, filepath) in enumerate(test_loader):\n",
    "            # расширение как идентификатор мапа\n",
    "            filepath = os.path.splitext(os.path.basename(filepath[0]))[0]\n",
    "            filepath = int(filepath)\n",
    "\n",
    "            image_var = torch.autograd.Variable(images, volatile=True)\n",
    "            y_pred = model(image_var)\n",
    "            # получение индекса с максимальным значением вероятности\n",
    "            smax = nn.Softmax()\n",
    "            smax_out = smax(y_pred)[0]\n",
    "            cat_prob = smax_out.data[0]\n",
    "            dog_prob = smax_out.data[1]\n",
    "            prob = dog_prob\n",
    "            if cat_prob > dog_prob:\n",
    "                prob = 1 - cat_prob\n",
    "            prob = np.around(prob, decimals=4)\n",
    "            prob = np.clip(prob, clip, 1-clip)\n",
    "            csv_map[filepath] += (prob / nb_aug)\n",
    "\n",
    "    sub_fn = submission_path + '{0}epoch_{1}clip_{2}runs'.format(epochs, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "        \n",
    "    print(\"Writing Predictions to CSV...\")\n",
    "    with open(sub_fn + '.csv', 'w') as csvfile:\n",
    "        fieldnames = ['id', 'label']\n",
    "        csv_w = csv.writer(csvfile)\n",
    "        csv_w.writerow(('id', 'label'))\n",
    "        for row in sorted(csv_map.items()):\n",
    "            csv_w.writerow(row)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем функцию, которая вычисляет и сохраняет среднее и текущее значение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем функцию, которая устанавливает скорость обучения для начального LR, \n",
    "разлагающегося на 10 каждые 30 периодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global lr\n",
    "    lr = lr * (0.1**(epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычислим точность нашего алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_actual, topk=(1, )):\n",
    "    maxk = max(topk)\n",
    "    batch_size = y_actual.size(0)\n",
    "\n",
    "    _, pred = y_pred.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageFolder(data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        images = []\n",
    "        for filename in sorted(glob.glob(test_path + \"*.jpg\")):\n",
    "            images.append('{}'.format(filename))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root, filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear(img):\n",
    "    width, height = img.size\n",
    "    m = random.uniform(-0.05, 0.05)\n",
    "    xshift = abs(m) * width\n",
    "    new_width = width + int(round(xshift))\n",
    "    img = img.transform((new_width, height), Image.AFFINE,\n",
    "                        (1, m, -xshift if m > 0 else 0, 0, 1, 0),\n",
    "                        Image.BICUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-36-bd9e599e12c2>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-bd9e599e12c2>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    models= models. __ dict __ [arch] (prerained = True)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def training(mode=\"train\", resume=False):\n",
    "    \n",
    "    global best_prec1\n",
    "    \n",
    "    for arch in archs:\n",
    "    # создали модель\n",
    "        print(\"=> Starting {0} on '{1}' model\".format(mode, arch))\n",
    "        model = models. __ dict __ [arch] (prerained = True)\n",
    "        # Не обновляем извлеченные функции, \n",
    "        #не связанные с классификатором, в предобученных сетях для параметра \n",
    "        param.parameters ():\n",
    "             param.requires_grad = False\n",
    "         # Заменим последний полностью подключенный уровень\n",
    "         # Параметры вновь построенных модулей имеют require_grad = True по умолчанию\n",
    "         #количество классов в этом случае -- resnet 101 -- это 2048 с двумя классами (кошки и собаки)\n",
    "         model.fc = nn.Linear (2048, 2)\n",
    "\n",
    "        if arch.startswith('alexnet') or arch.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "            \n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        # Загрузка данных\n",
    "        traindir = split_train_path\n",
    "        valdir = valid_path\n",
    "        testdir = test_path\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        train_loader = data.DataLoader(\n",
    "            datasets.ImageFolder(traindir,\n",
    "                                 transforms.Compose([\n",
    "                                     transforms.RandomSizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize,\n",
    "                                 ])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "\n",
    "        val_loader = data.DataLoader(\n",
    "            datasets.ImageFolder(valdir,\n",
    "                                 transforms.Compose([\n",
    "                                     transforms.Scale(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize,\n",
    "                                 ])),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True)\n",
    "\n",
    "        test_loader = data.DataLoader(\n",
    "            TestImageFolder(testdir,\n",
    "                            transforms.Compose([\n",
    "                                transforms.Scale(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                normalize,\n",
    "                            ])),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=False)\n",
    "        \n",
    "        \n",
    "        if mode == \"test\":\n",
    "            test(test_loader, model)\n",
    "            return\n",
    "        \n",
    "        # опеределение функции потерь\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "        if mode == \"validate\":\n",
    "            validate(val_loader, model, criterion, 0)\n",
    "            return\n",
    "\n",
    "        optimizer = optim.Adam(model.module.fc.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "            # обучение одного периода\n",
    "            train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "            # оценка\n",
    "            prec1 = validate(val_loader, model, criterion, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting train on 'resnet152' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth\" to /home/robert/.torch/models/resnet152-b121ed2d.pth\n",
      "100%|██████████| 241530880/241530880 [02:04<00:00, 1936481.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   * EPOCH 0 | Accuracy: 98.800 | Loss: 0.048\n",
      "   * EPOCH 1 | Accuracy: 99.000 | Loss: 0.041\n",
      "   * EPOCH 2 | Accuracy: 98.850 | Loss: 0.038\n",
      "   * EPOCH 3 | Accuracy: 98.850 | Loss: 0.039\n",
      "   * EPOCH 4 | Accuracy: 98.900 | Loss: 0.035\n",
      "   * EPOCH 5 | Accuracy: 98.950 | Loss: 0.036\n",
      "   * EPOCH 6 | Accuracy: 98.850 | Loss: 0.037\n",
      "   * EPOCH 7 | Accuracy: 98.950 | Loss: 0.036\n",
      "   * EPOCH 8 | Accuracy: 99.000 | Loss: 0.034\n",
      "   * EPOCH 9 | Accuracy: 98.900 | Loss: 0.036\n",
      "   * EPOCH 10 | Accuracy: 98.900 | Loss: 0.036\n",
      "   * EPOCH 11 | Accuracy: 98.800 | Loss: 0.039\n",
      "   * EPOCH 12 | Accuracy: 99.000 | Loss: 0.034\n",
      "   * EPOCH 13 | Accuracy: 98.800 | Loss: 0.041\n",
      "   * EPOCH 14 | Accuracy: 98.900 | Loss: 0.035\n",
      "   * EPOCH 15 | Accuracy: 99.000 | Loss: 0.037\n",
      "   * EPOCH 16 | Accuracy: 98.900 | Loss: 0.036\n",
      "   * EPOCH 17 | Accuracy: 99.150 | Loss: 0.033\n",
      "   * EPOCH 18 | Accuracy: 98.950 | Loss: 0.033\n",
      "   * EPOCH 19 | Accuracy: 98.900 | Loss: 0.037\n",
      "   * EPOCH 20 | Accuracy: 98.900 | Loss: 0.036\n",
      "   * EPOCH 21 | Accuracy: 98.750 | Loss: 0.040\n",
      "   * EPOCH 22 | Accuracy: 99.050 | Loss: 0.035\n",
      "   * EPOCH 23 | Accuracy: 99.100 | Loss: 0.033\n",
      "   * EPOCH 24 | Accuracy: 99.100 | Loss: 0.033\n",
      "   * EPOCH 25 | Accuracy: 99.100 | Loss: 0.034\n",
      "   * EPOCH 26 | Accuracy: 98.950 | Loss: 0.037\n",
      "   * EPOCH 27 | Accuracy: 99.100 | Loss: 0.036\n",
      "   * EPOCH 28 | Accuracy: 98.800 | Loss: 0.038\n",
      "   * EPOCH 29 | Accuracy: 98.900 | Loss: 0.032\n",
      "   * EPOCH 30 | Accuracy: 99.100 | Loss: 0.033\n",
      "   * EPOCH 31 | Accuracy: 98.900 | Loss: 0.033\n",
      "   * EPOCH 32 | Accuracy: 99.050 | Loss: 0.034\n",
      "   * EPOCH 33 | Accuracy: 99.000 | Loss: 0.034\n",
      "   * EPOCH 34 | Accuracy: 98.950 | Loss: 0.035\n"
     ]
    }
   ],
   "source": [
    "training(mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting validate on 'resnet152' model\n",
      "=> Loading checkpoint 'model_best.pth.tar'\n",
      "=> Loaded checkpoint (epoch 18)\n",
      "   * EPOCH 0 | Accuracy: 99.150 | Loss: 0.033\n"
     ]
    }
   ],
   "source": [
    "main(mode=\"validate\", resume='model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting test on 'resnet152' model\n",
      "=> Loading checkpoint 'model_best.pth.tar'\n",
      "=> Loaded checkpoint (epoch 18)\n",
      "   * Predicting on test augmentation 1\n",
      "   * Predicting on test augmentation 2\n",
      "   * Predicting on test augmentation 3\n",
      "Writing Predictions to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "main(mode=\"test\", resume='model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
